{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hidden Markov Models\n",
    "In this notebook we go through Hidden Markov Models (HHMs) from definition to implementation. We look at the following toy example (taken from Sebastian Thrun's lesson **Happy Grumpy Problem**):\n",
    "\n",
    "[hgp]: ./assets/rainsun_happygrumpy.png\n",
    "\n",
    "<center>\n",
    "\n",
    "![alt text][hgp]\n",
    "\n",
    "</center>\n",
    "where we can only observe if person is Happy or Grumpy but not the rain/sun (hidden-state).\n",
    "\n",
    "Let's define some notation for HMMs\n",
    "\n",
    "* $\\pi$: initial distribution of hidden state\n",
    "* $a_{ij}$: represents the transition from state $i$ to state $j$\n",
    "* $A = \\left(a_{ij}\\right)$: the set of state transition probabilites\n",
    "* $s_t$: the state at time $t$: \n",
    "$$s_t = i \\text{ with } i\\in\\left\\{\\text{rain, sun}\\right\\} $$\n",
    "* $o_t$: the observation at time $t$: \n",
    "$$o_t = k \\text{ with }k\\in\\left\\{\\text{happy, grumpy}\\right\\} $$\n",
    "* $b$: state output probability i.e\n",
    "$$b_i(k) \\text{ represents the probability of generating }k \\text{ in state }i$$\n",
    "* $B = \\left(b_i(k)\\right)$ the set of state output probabilities\n",
    "* a HMM is often represented by a tuple of $(\\pi, A, B)$\n",
    "\n",
    "The following notebook is organized as following\n",
    "\n",
    "* Generating rain-sun/happy-grumpy\n",
    "* Training HMMs using above generated observation to recover original probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating data\n",
    "We use the following parameters for our HMM\n",
    "* $\\pi = [0.5, 0.5]$ i.e \n",
    "$$P(s_1 = \\text{rain}) = P(s_1=\\text{sun}) = 0.5$$\n",
    "* transition matrix is given below\n",
    "$$\n",
    "\\begin{array}{lcc}\n",
    "            & s_{t+1}\\\\\n",
    "     s_t    & \\text{rain} & \\text{sun}\\\\\n",
    "\\text{rain} & 0.6  & 0.4\\\\\n",
    "\\text{sun}  & 0.2  & 0.8\n",
    "\\end{array}\n",
    "$$\n",
    "i.e \n",
    "\\begin{split}\n",
    "P(s_{t+1}=\\text{rain}\\left|\\ s_t=\\text{rain}\\right.) &= 0.6\\\\\n",
    "P(s_{t+1}=\\text{sun}\\left|\\ s_t=\\text{rain}\\right.) &= 0.4\\\\\n",
    "P(s_{t+1}=\\text{rain}\\left|\\ s_t=\\text{sun}\\right.) &= 0.2\\\\\n",
    "P(s_{t+1}=\\text{sun}\\left|\\ s_t=\\text{sun}\\right.) &= 0.8\n",
    "\\end{split}\n",
    "* state output probabilites is\n",
    "$$\n",
    "\\begin{array}{lcc}\n",
    "& o_t \\\\\n",
    "     s_t    & \\text{grumpy} & \\text{happy} \\\\\n",
    "\\text{rain} & 0.6  & 0.4\\\\\n",
    "\\text{sun}  & 0.1  & 0.9\n",
    "\\end{array}\n",
    "$$\n",
    "i.e\n",
    "\\begin{split}\n",
    "P(o_t=\\text{happy}\\left|\\ s_t=\\text{rain}\\right.) &= 0.4\\\\\n",
    "P(o_t=\\text{grumpy}\\left|\\ s_t=\\text{rain}\\right.) &= 0.6\\\\\n",
    "P(o_t=\\text{happy}\\left|\\ s_t=\\text{sun}\\right.) &= 0.9\\\\\n",
    "P(o_t=\\text{grumpy}\\left|\\ s_t=\\text{sun}\\right.) &= 0.1\n",
    "\\end{split}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# initial probabilities\n",
    "P0 = np.array([0.5, 0.5])\n",
    "\n",
    "# transition probabilities\n",
    "A = np.array([[0.6, 0.4],\n",
    "              [0.2, 0.8]])\n",
    "\n",
    "# output probabilities\n",
    "B = np.array([[0.7, 0.3],\n",
    "              [0.1, 0.9]])\n",
    "\n",
    "# map to state to index\n",
    "hstate_idx_map = {'R' :  0, 'S' :  1}\n",
    "hidx_state_map = { 0  : 'R', 1  : 'S'}\n",
    "\n",
    "ostate_idx_map = {'G' : 0, 'H' : 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implement utility functions to generate hidden-states and output-states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_hidden_states(P0, A, T):\n",
    "    uv = np.random.uniform(size=(T+1))\n",
    "    st = 0 if uv[0] < P0[0] else 1\n",
    "    hidden_states = [hidx_state_map[st]]\n",
    "    for i in range(1, T):\n",
    "        Pcond = A[st]\n",
    "        if uv[i] < Pcond[0]:            \n",
    "            st = 0\n",
    "        else:            \n",
    "            st = 1\n",
    "        # add new hidden-state\n",
    "        hidden_states.append(hidx_state_map[st])\n",
    "        \n",
    "    return hidden_states\n",
    "\n",
    "def generate_output_states(B, hidden_states):\n",
    "    uv = np.random.uniform(size=(len(hidden_states)))\n",
    "    out_states = []\n",
    "    for i,s in enumerate(hidden_states):\n",
    "        s_idx = hstate_idx_map[s]\n",
    "        Pcond = B[s_idx,:]\n",
    "        \n",
    "        if uv[i] < Pcond[0]:\n",
    "            out_states.append('G')\n",
    "        else:\n",
    "            out_states.append('H')\n",
    "    return out_states\n",
    "            \n",
    "def generate_hmm_seq(P0, A, B, T):\n",
    "    hidden_states = generate_hidden_states(P0, A, T)\n",
    "    out_states = generate_output_states(B, hidden_states)\n",
    "    return hidden_states, out_states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate 10-sequences, each sequences of length $T=20$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden-state 0-th:\tSSRRRSSSSS\n",
      "Observation  0-th:\tHHHHHHHHHH\n",
      "\n",
      "Hidden-state 1-th:\tRSSSSRSSSR\n",
      "Observation  1-th:\tHGGHHGHHGG\n",
      "\n",
      "Hidden-state 2-th:\tSSSSSSSSSS\n",
      "Observation  2-th:\tHHHHHHGHHH\n",
      "\n",
      "Hidden-state 3-th:\tRRSSSSSRRS\n",
      "Observation  3-th:\tGGHHHGHHHH\n",
      "\n",
      "Hidden-state 4-th:\tRRSSSSSRRS\n",
      "Observation  4-th:\tGGHHHHHGGH\n",
      "\n",
      "Hidden-state 5-th:\tRSSRSSRSRS\n",
      "Observation  5-th:\tGHHGHHHGHH\n",
      "\n",
      "Hidden-state 6-th:\tRSRRRRRRRR\n",
      "Observation  6-th:\tGHHGGGGHGG\n",
      "\n",
      "Hidden-state 7-th:\tSSSSSSSSSS\n",
      "Observation  7-th:\tHHHHHHHHHH\n",
      "\n",
      "Hidden-state 8-th:\tSSSSSSSRRS\n",
      "Observation  8-th:\tHGHHHHHGHH\n",
      "\n",
      "Hidden-state 9-th:\tRRSSSSRSSS\n",
      "Observation  9-th:\tHGHHGHHHHH\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hidden_datas   = []\n",
    "training_datas = []\n",
    "N = 10\n",
    "T = 10\n",
    "for i in range(N):\n",
    "    h_states, o_states = generate_hmm_seq(P0, A, B, T)\n",
    "    hidden_datas.append(h_states)\n",
    "    training_datas.append(o_states)\n",
    "\n",
    "for i in range(N):\n",
    "    print ('Hidden-state {}-th:\\t{}'.format(i, ''.join(hidden_datas[i])))\n",
    "    print ('Observation  {}-th:\\t{}\\n'.format(i, ''.join(training_datas[i])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HMM Decoding/Training\n",
    "Now, given above observation data, one can ask question: can we recover the parameters for our HMM i.e find \n",
    "\n",
    "$$(\\pi, A, B)\\text{ that maximizes the chance that we see above observation.}$$\n",
    "\n",
    "We will look at the following algorithm\n",
    "* [Viterbi algorithm](https://en.wikipedia.org/wiki/Viterbi_algorithm) for finding the most **likely** sequence of hidden states - called the **Viterbi path**\n",
    "* [Baumâ€“Welch algorithm](https://en.wikipedia.org/wiki/Baum%E2%80%93Welch_algorithm) for finding the unknown parameters of a HMM\n",
    "\n",
    "### Viterbi algorithm\n",
    "Suppose we know the model parameters i.e transition matrix $A$ and emission matrix $B$, and we observe a sequence of output $o_1,\\ldots,o_T$, we know want to find the hidden-state $s_1,\\ldots,s_T$ that most likely produce the observed sequence $o_t$.\n",
    "\n",
    "The above problem can be solved by Viterbi algorithm (see [wiki](https://en.wikipedia.org/wiki/Viterbi_algorithm) for more detail). The main idea is to find $s_1,\\ldots,s_T$ that maximizes the probability\n",
    "$$\n",
    "\\mathrm{arg}\\max_{s_1,\\ldots,s_T}P(o_1,\\ldots,o_T,s_1,\\ldots,s_T)\n",
    "$$\n",
    "\n",
    "Using Markov assumption for HMMs, we can write\n",
    "$$\n",
    "P(o_1,\\ldots,o_t,s_1,\\ldots,s_t) = p(o_t|s_t) \\times p(s_t|s_{t-1}) \\times P(o_1,\\ldots,o_{t-1},s_1,\\ldots,s_{t-1})\n",
    "$$\n",
    "Look at above recursive form, we can derive the following dynamics programming \n",
    "\n",
    "* $V_{1,s} = P(o_1,s_1=s)=P(o_1|s_1=s)\\times \\pi_s$\n",
    "* $V_{t,s} = \\max_{x\\in S} P(o_t|s_t=s)\\times P(s_t=s|s_{t-1}=x) \\times V_{t-1, x}$\n",
    "\n",
    "This allow to find the probability of the most probable state sequence $s_1,...,s_t$ that ends at state $s_t=s$.\n",
    "\n",
    "Let's implement Viterbi algorithm now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi_decode(observation, P0, A, B):\n",
    "    hidden_states = []\n",
    "    S = len(P0)\n",
    "    V = np.zeros(S)\n",
    "    # compute log-prob\n",
    "    logP0 = np.log(P0)\n",
    "    logA  = np.log(A)\n",
    "    logB  = np.log(B)\n",
    "    \n",
    "    # compute V_{1,s}\n",
    "    obs = ostate_idx_map[observation[0]]    \n",
    "    for i in range(S):\n",
    "        V[i] = logB[i, obs] + logP0[i]   # work in log-space since it might be very small\n",
    "    \n",
    "    prev_states = [] # to trace-back\n",
    "    # dp update V_{t,s}    \n",
    "    for i in range(1, len(observation)):\n",
    "        obs_i = ostate_idx_map[observation[i]]\n",
    "        \n",
    "        best_states = []\n",
    "        new_V = np.zeros(S)\n",
    "        for s in range(S):\n",
    "            new_v  = float('-inf')\n",
    "            best_x = None\n",
    "            for x in range(S):\n",
    "                v_sx = logA[x, s] + V[x]\n",
    "                if v_sx > new_v:\n",
    "                    new_v = v_sx\n",
    "                    best_x = x\n",
    "            \n",
    "            # keep best-x and update V\n",
    "            best_states.append(best_x)\n",
    "            new_V[s] = logB[s, obs_i] + new_v\n",
    "        \n",
    "        # update V\n",
    "        V = new_V\n",
    "        prev_states.append(best_states)\n",
    "        \n",
    "    # trace back\n",
    "    st = np.argmax(V)\n",
    "    hidden_states.insert(0, hidx_state_map[st])\n",
    "    for i in range(len(observation)-2,-1,-1):        \n",
    "        st = prev_states[i][st]\n",
    "        hidden_states.insert(0, hidx_state_map[st])\n",
    "    return hidden_states\n",
    "\n",
    "def decode_error(truth, decode):\n",
    "    N = len(truth)\n",
    "    err = 0\n",
    "    for i in range(N):\n",
    "        if truth[i] != decode[i]:\n",
    "            err += 1\n",
    "    return err/N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test sampe 0-th\n",
      "hidden = SSRRRSSSSS\n",
      "decode = SSSSSSSSSS\n",
      "decode-err = 0.30\n",
      "\n",
      "Test sampe 1-th\n",
      "hidden = RSSSSRSSSR\n",
      "decode = RRRSSSSSRR\n",
      "decode-err = 0.40\n",
      "\n",
      "Test sampe 2-th\n",
      "hidden = SSSSSSSSSS\n",
      "decode = SSSSSSSSSS\n",
      "decode-err = 0.00\n",
      "\n",
      "Test sampe 3-th\n",
      "hidden = RRSSSSSRRS\n",
      "decode = RRSSSSSSSS\n",
      "decode-err = 0.20\n",
      "\n",
      "Test sampe 4-th\n",
      "hidden = RRSSSSSRRS\n",
      "decode = RRSSSSSRRS\n",
      "decode-err = 0.00\n",
      "\n",
      "Test sampe 5-th\n",
      "hidden = RSSRSSRSRS\n",
      "decode = RSSSSSSSSS\n",
      "decode-err = 0.30\n",
      "\n",
      "Test sampe 6-th\n",
      "hidden = RSRRRRRRRR\n",
      "decode = RSSRRRRRRR\n",
      "decode-err = 0.10\n",
      "\n",
      "Test sampe 7-th\n",
      "hidden = SSSSSSSSSS\n",
      "decode = SSSSSSSSSS\n",
      "decode-err = 0.00\n",
      "\n",
      "Test sampe 8-th\n",
      "hidden = SSSSSSSRRS\n",
      "decode = SSSSSSSSSS\n",
      "decode-err = 0.20\n",
      "\n",
      "Test sampe 9-th\n",
      "hidden = RRSSSSRSSS\n",
      "decode = SSSSSSSSSS\n",
      "decode-err = 0.30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx in range(N):\n",
    "    decoded = viterbi_decode(training_datas[idx], P0, A, B)\n",
    "    print('Test sampe {}-th'.format(idx))\n",
    "    print('hidden = {}'.format(''.join(hidden_datas[idx])))\n",
    "    print('decode = {}'.format(''.join(decoded)))\n",
    "    print('decode-err = {:.2f}\\n'.format(decode_error(hidden_datas[idx], decoded)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baum-Welch algorithm\n",
    "Now, let's assume that we only observe the output-states without knowing about the parameters $(\\pi,A,B)$ and we want to recover $(\\pi, A,B)$. This is the goal of [Baum-Welch](https://en.wikipedia.org/wiki/Baum%E2%80%93Welch_algorithm).\n",
    "\n",
    "Let's implement it "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
