{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hidden Markov Models\n",
    "In this notebook we go through Hidden Markov Models (HHMs) from definition to implementation. We look at the following toy example (taken from Sebastian Thrun's lesson **Happy Grumpy Problem**):\n",
    "\n",
    "[hgp]: ./assets/rainsun_happygrumpy.png\n",
    "\n",
    "<center>\n",
    "\n",
    "![alt text][hgp]\n",
    "\n",
    "</center>\n",
    "where we can only observe if person is Happy or Grumpy but not the rain/sun (hidden-state).\n",
    "\n",
    "Let's define some notation for HMMs\n",
    "\n",
    "* $\\pi$: initial distribution of hidden state\n",
    "* $a_{ij}$: represents the transition from state $i$ to state $j$\n",
    "* $A = \\left(a_{ij}\\right)$: the set of state transition probabilites\n",
    "* $s_t$: the state at time $t$: \n",
    "$$s_t = i \\text{ with } i\\in\\left\\{\\text{rain, sun}\\right\\} $$\n",
    "* $o_t$: the observation at time $t$: \n",
    "$$o_t = k \\text{ with }k\\in\\left\\{\\text{happy, grumpy}\\right\\} $$\n",
    "* $b$: state output probability i.e\n",
    "$$b_i(k) \\text{ represents the probability of generating }k \\text{ in state }i$$\n",
    "* $B = \\left(b_i(k)\\right)$ the set of state output probabilities\n",
    "* a HMM is often represented by a tuple of $(\\pi, A, B)$\n",
    "\n",
    "The following notebook is organized as following\n",
    "\n",
    "* Generating rain-sun/happy-grumpy\n",
    "* Training HMMs using above generated observation to recover original probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generating data\n",
    "We use the following parameters for our HMM\n",
    "* $\\pi = [0.5, 0.5]$ i.e \n",
    "$$P(s_1 = \\text{rain}) = 0.4, P(s_1=\\text{sun}) = 0.6$$\n",
    "* transition matrix is given below\n",
    "$$\n",
    "\\begin{array}{lcc}\n",
    "            & s_{t}\\\\\n",
    "     s_{t+1}& \\text{rain} & \\text{sun}\\\\\n",
    "\\text{rain} & 0.6  & 0.2\\\\\n",
    "\\text{sun}  & 0.4  & 0.8\n",
    "\\end{array}\n",
    "$$\n",
    "i.e \n",
    "\\begin{split}\n",
    "P(s_{t+1}=\\text{rain}\\left|\\ s_t=\\text{rain}\\right.) &= 0.6\\\\\n",
    "P(s_{t+1}=\\text{sun}\\left|\\ s_t=\\text{rain}\\right.) &= 0.4\\\\\n",
    "P(s_{t+1}=\\text{rain}\\left|\\ s_t=\\text{sun}\\right.) &= 0.2\\\\\n",
    "P(s_{t+1}=\\text{sun}\\left|\\ s_t=\\text{sun}\\right.) &= 0.8\n",
    "\\end{split}\n",
    "* state output probabilites is\n",
    "$$\n",
    "\\begin{array}{lcc}\n",
    "& s_t \\\\\n",
    "     o_t    & \\text{rain} & \\text{sun} \\\\\n",
    "\\text{grumpy} & 0.7  & 0.1\\\\\n",
    "\\text{happy}  & 0.3  & 0.9\n",
    "\\end{array}\n",
    "$$\n",
    "i.e\n",
    "\\begin{split}\n",
    "P(o_t=\\text{happy}\\left|\\ s_t=\\text{rain}\\right.) &= 0.4\\\\\n",
    "P(o_t=\\text{grumpy}\\left|\\ s_t=\\text{rain}\\right.) &= 0.6\\\\\n",
    "P(o_t=\\text{happy}\\left|\\ s_t=\\text{sun}\\right.) &= 0.9\\\\\n",
    "P(o_t=\\text{grumpy}\\left|\\ s_t=\\text{sun}\\right.) &= 0.1\n",
    "\\end{split}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# initial probabilities\n",
    "P0 = np.array([0.4, 0.6])\n",
    "\n",
    "# transition probabilities\n",
    "A = np.array([[0.6, 0.2],\n",
    "              [0.4, 0.8]])\n",
    "\n",
    "# output probabilities\n",
    "B = np.array([[0.7, 0.1],\n",
    "              [0.3, 0.9]])\n",
    "\n",
    "logP0 = np.log(P0)\n",
    "logA  = np.log(A)\n",
    "logB  = np.log(B)\n",
    "\n",
    "# map to state to index\n",
    "hstate_idx_map = {'R' :  0, 'S' :  1}\n",
    "hidx_state_map = { 0  : 'R', 1  : 'S'}\n",
    "\n",
    "ostate_idx_map = {'G' : 0, 'H' : 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implement utility functions to generate hidden-states and output-states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_hidden_states(P0, A, T):\n",
    "    uv = np.random.uniform(size=(T))\n",
    "    st = 0 if uv[0] < P0[0] else 1\n",
    "    hidden_states = [hidx_state_map[st]]\n",
    "    for i in range(1, T):\n",
    "        Pcond = A[:, st]\n",
    "        if uv[i] < Pcond[0]:            \n",
    "            st = 0\n",
    "        else:            \n",
    "            st = 1\n",
    "        # add new hidden-state\n",
    "        hidden_states.append(hidx_state_map[st])\n",
    "        \n",
    "    return hidden_states\n",
    "\n",
    "def generate_output_states(B, hidden_states):\n",
    "    uv = np.random.uniform(size=(len(hidden_states)))\n",
    "    out_states = []\n",
    "    for i,s in enumerate(hidden_states):\n",
    "        s_idx = hstate_idx_map[s]\n",
    "        Pcond = B[:, s_idx]\n",
    "        \n",
    "        if uv[i] < Pcond[0]:\n",
    "            out_states.append('G')\n",
    "        else:\n",
    "            out_states.append('H')\n",
    "    return out_states\n",
    "            \n",
    "def generate_hmm_seq(P0, A, B, T):\n",
    "    hidden_states = generate_hidden_states(P0, A, T)\n",
    "    out_states = generate_output_states(B, hidden_states)\n",
    "    return hidden_states, out_states\n",
    "\n",
    "def generate_data(N, T):\n",
    "    hidden_datas   = []\n",
    "    training_datas = []\n",
    "    for i in range(N):\n",
    "        h_states, o_states = generate_hmm_seq(P0, A, B, T)\n",
    "        hidden_datas.append(h_states)\n",
    "        training_datas.append(o_states)\n",
    "    \n",
    "    return hidden_datas, training_datas\n",
    "\n",
    "def count(hidden_datas, training_datas):\n",
    "    init_count  = {}\n",
    "    trans_count = {}\n",
    "    out_count = {}\n",
    "    for h,t in zip(hidden_datas, training_datas):\n",
    "        init_count[h[0]] = init_count.get(h[0], 0) + 1\n",
    "        for i in range(len(h)):\n",
    "            oh = t[i] + h[i]\n",
    "            out_count[oh] = out_count.get(oh, 0) + 1\n",
    "            if i > 0:\n",
    "                ts = h[i] + h[i-1]\n",
    "                trans_count[ts] = trans_count.get(ts, 0) + 1\n",
    "    return init_count, trans_count, out_count        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate 10-sequences, each sequences of length $T=20$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N = 1000\n",
    "T = 100\n",
    "hidden_datas, training_datas = generate_data(N, T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init_count, trans_count, out_count = count(hidden_datas, training_datas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'R': 380, 'S': 620}\n",
      "Prob s_t+1=S|s_t=S = 0.80\n",
      "Prob s_t+1=S|s_t=R = 0.40\n",
      "Prob o_t=H|s_t=S   = 0.90\n",
      "Prob o_t=H|s_t=R   = 0.30\n"
     ]
    }
   ],
   "source": [
    "print(init_count)\n",
    "print('Prob s_t+1=S|s_t=S = {:.2f}'.format(trans_count['SS']/(trans_count['SS'] + trans_count['RS'])))\n",
    "print('Prob s_t+1=S|s_t=R = {:.2f}'.format(trans_count['SR']/(trans_count['SR'] + trans_count['RR'])))\n",
    "print('Prob o_t=H|s_t=S   = {:.2f}'.format(out_count['HS']/(out_count['HS'] + out_count['GS'])))\n",
    "print('Prob o_t=H|s_t=R   = {:.2f}'.format(out_count['HR']/(out_count['HR'] + out_count['GR'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden-state 0-th:\tRRSSS\n",
      "Observation  0-th:\tGGGHH\n",
      "\n",
      "Hidden-state 1-th:\tSSSSS\n",
      "Observation  1-th:\tHHHHH\n",
      "\n",
      "Hidden-state 2-th:\tSSSSS\n",
      "Observation  2-th:\tHHGHH\n",
      "\n",
      "Hidden-state 3-th:\tSSSRR\n",
      "Observation  3-th:\tHHHGG\n",
      "\n",
      "Hidden-state 4-th:\tSSSSS\n",
      "Observation  4-th:\tHHHHH\n",
      "\n",
      "Hidden-state 5-th:\tRSRSS\n",
      "Observation  5-th:\tGHGHH\n",
      "\n",
      "Hidden-state 6-th:\tRSSSS\n",
      "Observation  6-th:\tGHHGH\n",
      "\n",
      "Hidden-state 7-th:\tSSSSS\n",
      "Observation  7-th:\tHHHGH\n",
      "\n",
      "Hidden-state 8-th:\tRRRRS\n",
      "Observation  8-th:\tGGHGH\n",
      "\n",
      "Hidden-state 9-th:\tRSSSS\n",
      "Observation  9-th:\tHHGHH\n",
      "\n"
     ]
    }
   ],
   "source": [
    "N = 10\n",
    "T = 5\n",
    "hidden_datas, training_datas = generate_data(N, T)\n",
    "\n",
    "for i in range(N):\n",
    "    print ('Hidden-state {}-th:\\t{}'.format(i, ''.join(hidden_datas[i])))\n",
    "    print ('Observation  {}-th:\\t{}\\n'.format(i, ''.join(training_datas[i])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HMM Decoding/Training\n",
    "Now, given above observation data, one can ask question: can we recover the parameters for our HMM i.e find \n",
    "\n",
    "$$(\\pi, A, B)\\text{ that maximizes the chance that we see above observation.}$$\n",
    "\n",
    "We will look at the following algorithm\n",
    "* [Viterbi algorithm](https://en.wikipedia.org/wiki/Viterbi_algorithm) for finding the most **likely** sequence of hidden states - called the **Viterbi path**\n",
    "* [Baumâ€“Welch algorithm](https://en.wikipedia.org/wiki/Baum%E2%80%93Welch_algorithm) for finding the unknown parameters of a HMM\n",
    "\n",
    "### Viterbi algorithm\n",
    "Suppose we know the model parameters i.e transition matrix $A$ and emission matrix $B$, and we observe a sequence of output $o_1,\\ldots,o_T$, we know want to find the hidden-state $s_1,\\ldots,s_T$ that most likely produce the observed sequence $o_t$.\n",
    "\n",
    "The above problem can be solved by Viterbi algorithm (see [wiki](https://en.wikipedia.org/wiki/Viterbi_algorithm) for more detail). The main idea is to find $s_1,\\ldots,s_T$ that maximizes the probability\n",
    "$$\n",
    "\\mathrm{arg}\\max_{s_1,\\ldots,s_T}P(o_1,\\ldots,o_T,s_1,\\ldots,s_T)\n",
    "$$\n",
    "\n",
    "Using Markov assumption for HMMs, we can write\n",
    "$$\n",
    "P(o_1,\\ldots,o_t,s_1,\\ldots,s_t) = p(o_t|s_t) \\times p(s_t|s_{t-1}) \\times P(o_1,\\ldots,o_{t-1},s_1,\\ldots,s_{t-1})\n",
    "$$\n",
    "Look at above recursive form, we can derive the following dynamics programming \n",
    "\n",
    "* $V_{1,s} = P(o_1,s_1=s)=P(o_1|s_1=s)\\times \\pi_s$\n",
    "* $V_{t,s} = \\max_{x\\in S} P(o_t|s_t=s)\\times P(s_t=s|s_{t-1}=x) \\times V_{t-1, x}$\n",
    "\n",
    "This allow to find the probability of the most probable state sequence $s_1,...,s_t$ that ends at state $s_t=s$.\n",
    "\n",
    "Let's implement Viterbi algorithm now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def viterbi_path_eval(hidden, observation, logP0, logA, logB, debug = False):\n",
    "    # get state at t = 0\n",
    "    s_0   = hstate_idx_map[hidden[0]]\n",
    "    obs_0 = ostate_idx_map[observation[0]]   \n",
    "    \n",
    "    path_val = logP0[s_0] + logB[obs_0, s_0]\n",
    "    if debug:\n",
    "        print ('step {:2d}-th path={}'.format(0, path_val))\n",
    "    \n",
    "    # continue the path\n",
    "    prev_s = s_0\n",
    "    for i in range(1, len(hidden)):\n",
    "        s   = hstate_idx_map[hidden[i]]\n",
    "        obs = ostate_idx_map[observation[i]]   \n",
    "        path_val += logA[s, prev_s] + logB[obs, s]\n",
    "        \n",
    "        if debug:\n",
    "            print ('step {:2d}-th path={}'.format(i, path_val))\n",
    "        \n",
    "        prev_s = s\n",
    "    \n",
    "    return path_val\n",
    "\n",
    "def viterbi_decode(observation, logP0, logA, logB, debug = False):\n",
    "    hidden_states = []\n",
    "    S = len(P0)\n",
    "    \n",
    "#     print('logA {}'.format(logA))\n",
    "#     print('\\nlogB {}\\n'.format(logB))\n",
    "    # compute V_{1,s}\n",
    "    obs = ostate_idx_map[observation[0]]    \n",
    "    V = logB[obs] + logP0\n",
    "    if debug:\n",
    "        print('step {:2d}-th V={}\\n'.format(0, V))\n",
    "        print('logP0 = {}'.format(logP0))\n",
    "        print('logA = {}'.format(logA))\n",
    "        print('logB = {}'.format(logB))\n",
    "    \n",
    "    prev_states = [] # to trace-back\n",
    "    # dp update V_{t,s}    \n",
    "    for i in range(1, len(observation)):\n",
    "        obs_i = ostate_idx_map[observation[i]]\n",
    "        \n",
    "        # for each s, we compute trans_prob = logA[s, x] + V[x] for all x in S\n",
    "        # this's equivalent to add each row of logA by V\n",
    "        trans_prob = logA + V\n",
    "                \n",
    "        # get best x for each row\n",
    "        best_states = np.argmax(trans_prob, axis=1)\n",
    "\n",
    "        # update V with new best_states new_V[s] = logB[obs_i, s] + max_x logA[s,x] + V[x]\n",
    "        V = logB[obs_i] + trans_prob[np.arange(S), best_states]\n",
    "        \n",
    "        if debug:\n",
    "            print('\\nstep {:2d}-th\\ntrans_prob={}'.format(i, trans_prob))\n",
    "            print('best_states={}'.format(best_states))\n",
    "            print('V={}'.format(V))\n",
    "            \n",
    "        # keep track best-states to back-track\n",
    "        prev_states.append(best_states)\n",
    "        \n",
    "    # trace back\n",
    "    st = np.argmax(V)\n",
    "    hidden_states.insert(0, hidx_state_map[st])\n",
    "    for i in range(len(observation)-2,-1,-1):        \n",
    "        st = prev_states[i][st]\n",
    "        hidden_states.insert(0, hidx_state_map[st])\n",
    "    return hidden_states\n",
    "\n",
    "def decode_error(truth, decode):\n",
    "    N = len(truth)\n",
    "    err = 0\n",
    "    for i in range(N):\n",
    "        if truth[i] != decode[i]:\n",
    "            err += 1\n",
    "    return err/N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden   = RSSSS\n",
      "path-val = -6.32\n",
      "\n",
      "decode   = SSSSS\n",
      "path-val = -4.13\n",
      "\n",
      "decode-err = 0.20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "This is quite interesting where it fails to find the most probable\n",
    "Test sampe 3-th\n",
    "obs      = GGHHH\n",
    "hidden   = RRSRR\n",
    "path-val = -8.19\n",
    "decode   = RRSSS\n",
    "path-val = -10.41\n",
    "decode-err = 0.40\n",
    "'''\n",
    "\n",
    "idx = 9\n",
    "decoded = viterbi_decode(training_datas[idx], logP0, logA, logB)\n",
    "print('hidden   = {}\\npath-val = {:.2f}\\n'.format(''.join(hidden_datas[idx]), viterbi_path_eval(hidden_datas[idx],\n",
    "                                                                                              training_datas[idx],\n",
    "                                                                                             logP0, logA, logB)))\n",
    "print('decode   = {}\\npath-val = {:.2f}\\n'.format(''.join(decoded), viterbi_path_eval(decoded,\n",
    "                                                                                    training_datas[idx],\n",
    "                                                                                    logP0, logA, logB)))\n",
    "print('decode-err = {:.2f}\\n'.format(decode_error(hidden_datas[idx], decoded)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test sampe 0-th\n",
      "obs      = GGGHH\n",
      "hidden   = RRSSS\n",
      "path-val = -6.02\n",
      "decode   = RRRSS\n",
      "path-val = -4.36\n",
      "decode-err = 0.20\n",
      "\n",
      "Test sampe 1-th\n",
      "obs      = HHHHH\n",
      "hidden   = SSSSS\n",
      "path-val = -1.93\n",
      "decode   = SSSSS\n",
      "path-val = -1.93\n",
      "decode-err = 0.00\n",
      "\n",
      "Test sampe 2-th\n",
      "obs      = HHGHH\n",
      "hidden   = SSSSS\n",
      "path-val = -4.13\n",
      "decode   = SSSSS\n",
      "path-val = -4.13\n",
      "decode-err = 0.00\n",
      "\n",
      "Test sampe 3-th\n",
      "obs      = HHHGG\n",
      "hidden   = SSSRR\n",
      "path-val = -4.11\n",
      "decode   = SSSRR\n",
      "path-val = -4.11\n",
      "decode-err = 0.00\n",
      "\n",
      "Test sampe 4-th\n",
      "obs      = HHHHH\n",
      "hidden   = SSSSS\n",
      "path-val = -1.93\n",
      "decode   = SSSSS\n",
      "path-val = -1.93\n",
      "decode-err = 0.00\n",
      "\n",
      "Test sampe 5-th\n",
      "obs      = GHGHH\n",
      "hidden   = RSRSS\n",
      "path-val = -5.61\n",
      "decode   = RRRSS\n",
      "path-val = -5.21\n",
      "decode-err = 0.20\n",
      "\n",
      "Test sampe 6-th\n",
      "obs      = GHHGH\n",
      "hidden   = RSSSS\n",
      "path-val = -5.48\n",
      "decode   = RSSSS\n",
      "path-val = -5.48\n",
      "decode-err = 0.00\n",
      "\n",
      "Test sampe 7-th\n",
      "obs      = HHHGH\n",
      "hidden   = SSSSS\n",
      "path-val = -4.13\n",
      "decode   = SSSSS\n",
      "path-val = -4.13\n",
      "decode-err = 0.00\n",
      "\n",
      "Test sampe 8-th\n",
      "obs      = GGHGH\n",
      "hidden   = RRRRS\n",
      "path-val = -5.74\n",
      "decode   = RRRRS\n",
      "path-val = -5.74\n",
      "decode-err = 0.00\n",
      "\n",
      "Test sampe 9-th\n",
      "obs      = HHGHH\n",
      "hidden   = RSSSS\n",
      "path-val = -6.32\n",
      "decode   = SSSSS\n",
      "path-val = -4.13\n",
      "decode-err = 0.20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx in range(N):\n",
    "    decoded = viterbi_decode(training_datas[idx], logP0, logA, logB)\n",
    "    print('Test sampe {}-th'.format(idx))\n",
    "    print('obs      = {}'.format(''.join(training_datas[idx])))\n",
    "    print('hidden   = {}\\npath-val = {:.2f}'.format(''.join(hidden_datas[idx]), viterbi_path_eval(hidden_datas[idx],\n",
    "                                                                                              training_datas[idx],\n",
    "                                                                                             logP0, logA, logB)))\n",
    "    print('decode   = {}\\npath-val = {:.2f}'.format(''.join(decoded), viterbi_path_eval(decoded,\n",
    "                                                                                    training_datas[idx],\n",
    "                                                                                    logP0, logA, logB)))\n",
    "    print('decode-err = {:.2f}\\n'.format(decode_error(hidden_datas[idx], decoded)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baum-Welch algorithm\n",
    "Now, let's assume that we only observe the output-states without knowing about the parameters $(\\pi,A,B)$ and we want to recover $(\\pi, A,B)$. This is the goal of [Baum-Welch](https://en.wikipedia.org/wiki/Baum%E2%80%93Welch_algorithm) which contains the following steps\n",
    "\n",
    "1. Initialize $\\theta=(\\pi, A, B)$ by random where\n",
    "\\begin{split}\n",
    "\\pi(s) &= P(s_1=s)\\\\\n",
    "A(s,x) &= P(s_{t}=s | s_{t-1} = x)\\\\\n",
    "B(o,s) &= P(o_t=o|s_t=s)\n",
    "\\end{split}\n",
    "\n",
    "2. Forward procedure, let \n",
    "$$\n",
    "    \\alpha_s(t) = P\\left(o_1,\\ldots,o_t,s_t=s|\\theta\\right)\n",
    "$$\n",
    "the probability of seeing $(o_1,\\ldots,o_t)$ **and** being in hidden-state $s$ at time $t$. This is found recursively (via total-probability & Bayes-rule)\n",
    "\\begin{split}\n",
    "\\alpha_s(1) &= \\pi(s)\\times B(o_1, s)\\\\\n",
    "\\alpha_s(t) &= B(o_t,s) \\times \\sum_{x\\in S} A(s, x) \\times \\alpha_x(t-1)\n",
    "\\end{split}\n",
    "\n",
    "3. Backward procedure, let\n",
    "$$\n",
    "\\beta_s(t) = P(o_{t+1},\\ldots, o_T | s_t = s;\\theta)\n",
    "$$\n",
    "that is the probability of the ending partial observation is $o_{t+1},\\ldots, o_{T}$ **given** the hidden-state at time $t$ equals $s$. We calculate $\\beta_s(t)$ as\n",
    "\\begin{split}\n",
    "\\beta_s(T) &= 1\\\\\n",
    "\\beta_s(t) &= \\sum_{x\\in S} \\beta_x(t+1) \\times B(o_{t+1}, x) \\times A(x,s)\n",
    "\\end{split}\n",
    "Let's derive above \n",
    "\\begin{split}\n",
    "\\beta_s(t) &= \\frac{P(o_{t+1},\\ldots, o_T , s_t = s|\\theta)}{P(s_t=s|\\theta)}\\\\\n",
    "&= \\frac{\\sum_{x\\in S} P(o_{t+1},\\ldots, o_T , s_t = s, s_{t+1}=x|\\theta) }{P(s_t=s|\\theta)}\\\\\n",
    "&= \\frac{\\sum_{x\\in S} P(o_{t+1},\\ldots, o_T | s_t = s, s_{t+1}=x;\\theta)\\times P(s_t = s, s_{t+1}=x|\\theta) }{P(s_t=s|\\theta)}\\\\\n",
    "&= \\sum_{x\\in S} P(o_{t+1},\\ldots, o_T |s_{t+1}=x;\\theta)\\times P(s_{t+1}=x|s_t = s;\\theta) \\\\\n",
    "&= \\sum_{x\\in S} P(o_{t+2},\\ldots, o_T |s_{t+1}=x;\\theta)\\times P(o_{t+1}|s_{t+1}=x;\\theta)\\times  P(s_{t+1}=x|s_t = s;\\theta)\\\\\n",
    "&= \\sum_{x\\in S} \\beta_x(t+1)\\times B(o_{t+1}, x)\\times A(x,s)\n",
    "\\end{split}\n",
    "\n",
    "4. Update, let\n",
    "$$\n",
    "\\gamma_s(t) = P(s_t=s|o_1,\\ldots,o_T;\\theta) = \\frac{P(s_t=s, o_1,\\ldots,o_T|\\theta)}{P(o_1,\\ldots,o_T|\\theta)}\n",
    "$$\n",
    "\n",
    "Let's implement it "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
